{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14823614-b416-4a45-b48b-60205235e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import sys\n",
    "from torch import nn, Tensor\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from model import TransformerModel, generate_square_subsequent_mask\n",
    "from data_utils import data_process, batchify, get_batch\n",
    "from train import train_epoch, evaluate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"torch_version: {torch.__version__}\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae4da6-0835-4d9a-9ad3-4d1b78590733",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c68323-68aa-4c5e-87a8-a0d9a34c0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "train_iter = WikiText2(split=\"train\")\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter, vocab, tokenizer)\n",
    "val_data = data_process(val_iter, vocab, tokenizer)\n",
    "test_data = data_process(test_iter, vocab, tokenizer)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size, device)\n",
    "val_data = batchify(val_data, eval_batch_size, device)\n",
    "test_data = batchify(test_data, eval_batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83ad05-f0d3-461c-8b76-6065b9d247f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {i: x for i, x in enumerate(vocab.get_itos())}\n",
    "def to_sentence(x):\n",
    "    return \" \".join([idx2word[int(idx)] for idx in x])\n",
    "\n",
    "i = 50\n",
    "x = to_sentence(test_data[i:i+30][:, 0])\n",
    "y = to_sentence(test_data[i+1:i+1+30][:, 0])\n",
    "print(f\"{x}\\n\\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee63ab-760f-4c83-be89-1158eed54017",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef08a6b-b4ce-4993-8cfc-f4a4d9efdaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)\n",
    "emsize = 200\n",
    "d_hid = 200\n",
    "nlayers = 2\n",
    "nhead = 2\n",
    "dropout = 0.2\n",
    "\n",
    "model =  TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccb8909-25ff-4173-9978-a4d3dab5682a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e325a68-bf42-4f39-aa58-d09d34fc2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15e7cc-e839-4d7f-84ac-c7e83590ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 2\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_epoch(train_data, model, criterion, optimizer, scheduler, ntokens, epoch, \n",
    "                log_interval=10, device=device)\n",
    "    \n",
    "    val_loss = evaluate(val_data, model, criterion, ntokens, device=device) \n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2b0b0-0422-4ecb-8084-ff817b124f16",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad816f40-fcaf-4f9f-8012-253ed8c553e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, best_model, criterion, ntokens, device=device) \n",
    "test_ppl = math.exp(test_loss)\n",
    "\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b3406-c6fa-4307-bcc0-9b7c9137d13a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
